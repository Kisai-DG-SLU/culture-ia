# üìã Revue de Projet Finale : Culture IA (√âtapes 1 √† 6)

**Document de pr√©paration au mentorat et √† la soutenance**
**Statut :** üöÄ Pr√™t pour livraison / Architecture Industrialis√©e

---

## üèóÔ∏è √âtape 1 : Environnement & Qualit√© (Valid√©)
- **Objectif** : Cadre reproductible et s√©curis√©.
- **Industrialisation** : Utilisation d'un `Makefile` pour automatiser l'installation et les tests.
- **Qualit√© de code** : Mise en place de **Pre-commit hooks** (Black pour le formatage, Pylint pour la qualit√©). Le code est bloqu√© s'il n'est pas "propre".
- **CI/CD** : Pipeline GitHub Actions qui valide les tests et la couverture (> 70%) √† chaque push.
- **S√©curit√©** : Secrets g√©r√©s via `.env` (non versionn√©) et mod√®le `.env.template`.

---

## üßπ √âtape 2 : Collecte et Pr√©traitement (Valid√© & Optimis√©)
- **Objectif** : R√©cup√©rer et structurer les donn√©es.
- **API OpenAgenda V2** : Param√©trage avanc√© (`includeFields`, `relative`) pour garantir la r√©cup√©ration des √©v√©nements **futurs (2025-2026)**.
- **R√©solution de Probl√®me (Storytelling Soutenance)** : Expliquer comment l'analyse des r√©ponses API vides a conduit √† d√©couvrir que les dates √©taient masqu√©es par d√©faut, et comment cela a √©t√© corrig√©.
- **Nettoyage** : Structuration s√©mantique des donn√©es (Titre + Description + Lieu + Dates) pour maximiser la pertinence.
- **üí° Note Technique (Justification)** : Pas de "Pandas" car le JSON imbriqu√© d'OpenAgenda est plus efficace √† traiter en Python natif (plus l√©ger et plus rapide pour ce volume).

---

## üß† √âtape 3 : Vectorisation & Indexation (Valid√©)

### ‚úÇÔ∏è Strat√©gie de Chunking
Notre approche est **hybride et s√©curis√©e** :
1.  **Par d√©faut (S√©mantique)** : 1 √âv√©nement = 1 Vecteur. Pr√©serve l'unit√© de l'information.
2.  **S√©curit√© (Technique)** : `RecursiveCharacterTextSplitter` (max 1000 chars) avec un **recouvrement (overlap) de 200 caract√®res**.
    - *Pourquoi le recouvrement ?* Pour garantir qu'aucune information n'est perdue √† la "fronti√®re" entre deux blocs. Cela permet de conserver le contexte s√©mantique des phrases qui seraient autrement coup√©es en deux.
    - *Impact* : Meilleure pr√©cision lors de la recherche (Retrieval) car le sens est maintenu m√™me sur les textes longs.

### üìç Comprendre l'Embedding (Le concept pour le Jury)
*Question : C'est quoi un embedding ? Faut-il comprendre le r√©seau neuronal ?*
**R√©ponse :** Non, c'est un **GPS des mots**.
- Le mod√®le transforme un texte en coordonn√©es math√©matiques.
- Les textes ayant le m√™me **sens** sont g√©ographiquement **proches** dans cet espace.
- *Exemple* : "Cuisine sauvage" et "Plantes comestibles" sont voisins math√©matiquement.

### üíé Architecture Hybride (Mistral / Local)
Le syst√®me est capable de fonctionner de deux mani√®res pour la vectorisation :
1.  **Principal (Cloud)** : Mistral AI Embeddings.
2.  **Fallback (Local)** : HuggingFace `all-MiniLM-L6-v2` (**100% Local sur le Mac**).
    - *Avantages* : Gratuit√©, Confidentialit√©, Fonctionne hors-ligne.

---

## ü§ñ √âtape 4 & 5 : Chatbot RAG & API (Valid√©)
- **Orchestration** : LangChain lie FAISS (le biblioth√©caire) et Mistral (le cerveau).
- **Prompt Engineering** :
    - **Persona** : "Tu es un assistant expert..."
    - **Anti-Hallucination** : "R√©ponds UNIQUEMENT avec le contexte."
    - **Conscience Temporelle** : Injection de la date du jour (`"Nous sommes le..."`) pour √©viter de proposer des √©v√©nements pass√©s.
- **API FastAPI** : Interface moderne avec documentation Swagger (`/docs`) et endpoint de reconstruction (`/rebuild`) avec Hot-Reload (mise √† jour sans red√©marrage).
- **√âvaluation Ragas** : Mesure scientifique de la fid√©lit√© et de la pertinence.
    - *R√©sultats* : Fid√©lit√© (~82%), Rappel (~75%).
    - *Analyse Pr√©cision (50%)* : Identifi√© comme un artefact structurel (Dataset de 2 √©v√©nements vs k=2). Le "bruit" est math√©matiquement in√©vitable ici mais serait dilu√© avec plus de donn√©es.

---

## üì¶ √âtape 6 : Docker & D√©mo Live

### üê≥ Docker
- Le conteneur est "intelligent" (`entrypoint.sh`) : il d√©tecte l'absence de donn√©es au d√©marrage et lance automatiquement la collecte et l'indexation.

### üß™ D√©mo du "Mode Local" (HuggingFace)
Pour prouver la robustesse (fallback) sans toucher aux fichiers de config :

**La commande magique :**
```bash
MISTRAL_API_KEY=none PYTHONPATH=. .venv/bin/python src/core/vectorstore.py
```

**L'explication :**
"Je simule une panne de l'API Mistral. Le syst√®me le d√©tecte et bascule instantan√©ment sur le mod√®le local. Vous voyez les barres de t√©l√©chargement (la premi√®re fois) puis l'indexation r√©ussie."

---

## üí° Conseils pour la Soutenance

1.  **Lancez Docker en avance** : `make docker-build` d√®s maintenant, puis `make docker-run` juste avant la d√©mo.
2.  **Le RAG expliqu√© √† un enfant** : "C'est comme un examen √† livre ouvert. Le LLM (l'√©l√®ve) ne r√©pond pas par c≈ìur, il doit chercher la r√©ponse dans le manuel (l'Agenda) que je lui donne."
3.  **Mettez en avant l'industrialisation** : Insistez sur la CI/CD, les tests automatiques et le Docker. C'est ce qui diff√©rencie un "bricolage" d'un projet "Ing√©nieur".
